{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1 style=\"font-size: 32px;\">Wrangle and Analyze Data</h1>\n",
    "    <h2>Abdulrahman Mohammed Alobaidy</h2>\n",
    "    <h3>Cohort 9</h3>\n",
    "    <h3>Data Analyst Nanodegree Program</h3>\n",
    "    <h4>Email: <a href=\"mailto:AbdulrahmanAlobaidy2001@gmail.com\">AbdulrahmanAlobaidy2001@gmail.com</a></h4>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we have the [**WeRateDogs**](https://twitter.com/dog_rates) (**@dog_rates**) tweets and data that we must wrangle in order for the data to be suitable for analysis purposes.\n",
    "\n",
    "[**WeRateDogs**](https://twitter.com/dog_rates) is a twitter page that posts photos of pet dogs, each with a rating, sometimes with a dog stage and the dog's name, here, we will attempt to **gather** data related to the page from multiple sources, **assess** this data's quality and tidiness, and **clean** the data and fix the problems found during the assessment stage, **Data Wrangling** essentially, in order to prepare the data for the ultimate goal, which is **data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AbdulrahmanAlobaidy/DAND-project-4/blob/main/tweet.png?raw=true\" alt=\"WeRateDogs Tweet\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be gathering data from three sources, text file, request from the internet and finally read from API, for the last one, I couldn't get access to the Twitter API, thus I opted to read the data from a text file.\n",
    "\n",
    "### 1. `twitter_archive_enhanced.csv`\n",
    "First, we read a `csv` file named `twitter_archive_enhanced.csv`, which contains information about the tweets, into a Pandas DataFrame named `df_enhanced`.\n",
    "### 2. `image_predictions.tsv`\n",
    "Then, we read a `tsv` file named `image_predictions.tsv` using the Python's Requests library, store it into a Pandas DataFrame named `df_image_predictions`, then saved it locally.\n",
    "### 3. `tweet-json.txt`\n",
    "Finally, we read the API data that provides more information about the tweets from the first step, we read the `tweet-json.txt` file line by line, each line is a `json` string, then we parse and store every line in a list to be later imported into a Pandas DataFrame named `df_api`, at last, this DataFrame is exported as a file named `tweet_json.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to first assess the datasets visually.\n",
    "\n",
    "---\n",
    "### 1. `df_enhanced`\n",
    "We start by displaying the first five rows from the first DataFrame, `df_enhanced`\n",
    "\n",
    "The first thing we notice is that the `rating_denominator` column is completely redundant, that is because the value is constant and is always a **10**.\n",
    "\n",
    "\n",
    "The second thing we notice is that the dog stages are distributed into four columns, (`doggo`, `floofer`, `pupper`, `puppo`), and need to be combined into a single column.\n",
    "\n",
    "Finally, there is no rating column that calculates the result of the numerator divided by the denominator, we would need that for analysis purposes.\n",
    "\n",
    "### 2. `df_image_predictions`\n",
    "Now we display the first five rows from the `df_image_predictions` DataFrame.\n",
    "\n",
    "This one seemed okay.\n",
    "\n",
    "### 3. `df_api`\n",
    "At last, we also display the first five rows from the `df_api` DataFrame.\n",
    "\n",
    "This one also seemed fine, but there were a lot of collapsed columns, so we need to further examine it programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to assess the datasets programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `df_enhanced`\n",
    "We use the Pandas' `info` method on `df_enhanced` in order to take a look at the columns, their non-nulls and datatypes.\n",
    "\n",
    "We noticed the following:\n",
    "\n",
    "* The `timestamp` column is of datatype `object` instead of `datatime`.\n",
    "\n",
    "* The retweets need to be removed from the dataset in order to analyze only the **WeRateDogs** tweets, thus we don't need the `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp` columns.\n",
    "\n",
    "One of the **Key Points** in the **Project Motivation** section mentioned that there could be a problem with the `rating_numerator` and `rating_denominator` columns, so we used Pandas' `value_counts` method in order to look at the instances of values in the `rating_numerator` column only, since the `rating_denominator` column was to be removed anyways.\n",
    "\n",
    "We did find some wrong-seeming values such as 420, 144, 960 and etc... .\n",
    "\n",
    "### 2. `df_image_predictions`\n",
    "We also used the `info` method on `df_image_predictions`, fortunatly, every thing seemed good.\n",
    "\n",
    "### 3. `df_api`\n",
    "Again, we first used the `info` method, and we noticed the following:\n",
    "\n",
    "* The `created_at` column is of datatype `object` instead of `datetime`.\n",
    "* Both the `possible_sensitive` as well as the `possibly_sensitive_appealable` columns are of datatype `object` where they should be `bool`.\n",
    "\n",
    "Then we ran the `value_counts` method on the `lang` column, and decided that this column's datatype to be changed from `object` to `category`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issues:\n",
    "1. The group stages columns (`doggo`, `floofer`, `pupper`, `puppo`) need to be melted into a single column.\n",
    "2. Merge all of the three DataFrames into a single master dataset.\n",
    "\n",
    "### Quality Issues:\n",
    "1. The `rating_denominator` column in `df_enhanced` is redundant.\n",
    "2. Remove retweets from `df_enhanced`.\n",
    "3. Remove the `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` and `retweeted_status_timestamp` columns from `df_enhanced`.\n",
    "4. `timestamp` column of `df_enhanced` should be of datatype `datetime`.\n",
    "5. Extract the `rating_numerator` from `text` in `df_enhanced`.\n",
    "6. Convert `rating_numerator`'s datatype to `float`.\n",
    "7. Remove records from `df_enhanced` with wrong `rating_numerator` values.\n",
    "8. Missing `rating` column.\n",
    "9. `created_at` column in `df_api` should be of datatype `datetime`.\n",
    "10. `possibly_sensitive` column in `df_api` should be of datatype `bool`\n",
    "11. `possibly_sensitive_appealable` column in `df_api` should be of datatype `bool`.\n",
    "12. `lang` column in `df_api` should be of datatype `category`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we create copies of the three DataFrames to preserve the originals in case we accidentally corrupt the one or more of the DataFrames, we name them, `df_enhanced_copy`, `df_image_predictions_copy` and `df_api_copy` accordingly.\n",
    "\n",
    "\n",
    "Now we address the aforementioned issues.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. The group stages columns (`doggo`, `floofer`, `pupper`, `puppo`) need to be melted into a single column.\n",
    "\n",
    "At first, we replace the **None**s with **NaN**s, then we combine the four columns by joining them with a comma, and we replace the empty strings with **NaN**s, since some of the records have no dog stage, we store the combined data to a new column called `dog_stage` and we drop the four columns.\n",
    "\n",
    "\n",
    "#### 2. The `rating_denominator` column in `df_enhanced` is redundant.\n",
    "\n",
    "Here, we just drop the `rating_denominator` column for being always **10**.\n",
    "\n",
    "\n",
    "#### 3. Remove retweets from `df_enhanced`.\n",
    "\n",
    "We remove the records in `df_enhanced_copy` with nulls in the `in_reply_to_status_id` column.\n",
    "\n",
    "\n",
    "#### 4. Remove the `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` and \n",
    "\n",
    "#### `retweeted_status_timestamp` columns from `df_enhanced`.\n",
    "\n",
    "We remove the aforementioned columns from `df_enhanced_copy`.\n",
    "\n",
    "\n",
    "#### 5. `timestamp` column of `df_enhanced` should be of datatype `datetime`.\n",
    "\n",
    "We change the `timestamp` column datatype into `datetime` using Pandas' `to_datetime` method.\n",
    "\n",
    "\n",
    "#### 6. Extract the `rating_numerator` from `text` in `df_enhanced`.\n",
    "\n",
    "We extract the numerator from the `text` column in `df_enhanced_copy`, then store it in `rating_numerator`.\n",
    "\n",
    "\n",
    "#### 7. Convert `rating_numerator`'s datatype to `float`.\n",
    "\n",
    "We convert the `rating_numerator`'s datatype to `float`.\n",
    "\n",
    "#### 8. Remove records from `df_enhanced` with wrong `rating_numerator` values.\n",
    "\n",
    "We get the indices where the `rating_numerator` is 1776 or 420, then drop them from `df_enhanced_copy`.\n",
    "\n",
    "\n",
    "#### 9. Missing `rating` column.\n",
    "\n",
    "Divide the `rating_numerator` by **10** and store it in `rating` column.\n",
    "\n",
    "\n",
    "#### 10. `created_at` column in `df_api` should be of datatype `datetime`.\n",
    "\n",
    "Change the `created_at` column in `df_api_copy` to `datetime` using Pandas' `to_datetime` method.\n",
    "\n",
    "#### 11. `possibly_sensitive` column in `df_api` should be of datatype `bool`\n",
    "\n",
    "Change the datatype of the `possibly_sensitive` column in `df_api_copy` to `bool`.\n",
    "\n",
    "#### 12. `possibly_sensitive_appealable` column in `df_api` should be of datatype `bool`.\n",
    "\n",
    "Change the datatype of the `possibly_sensitive_appealable` column in `df_api_copy` to `bool`.\n",
    "\n",
    "#### 13. `lang` column in `df_api` should be of datatype `category`.\n",
    "\n",
    "Change the datatype of the `lang` column in `df_api_copy` to `category`.\n",
    "\n",
    "#### 14. Merge all of the three DataFrames into a single master dataset.\n",
    "\n",
    "We keep only the needed columns from `df_api_copy`, then use Pandas' `merge` method to combine all of the three cleaned DataFrames into a single master dataset, then save this dataset into a file called `twitter_archive_master.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the code and datasets for this project are included on a [Github Repository](https://github.com/AbdulrahmanAlobaidy/DAND-project-4) dedicated for this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
